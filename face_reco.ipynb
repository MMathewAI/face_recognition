{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_map(x):\n",
    "    if x=='sachin':\n",
    "        return 0\n",
    "    if x=='obama':\n",
    "        return 1\n",
    "    if x=='messi':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(x):\n",
    "    if x==0:\n",
    "        return 'sachin'\n",
    "    if x==1:\n",
    "        return 'obama'\n",
    "    if x==2:\n",
    "        return 'messi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROI - region of interest\n",
    "# we need to identify just the face and no need of any background ie ROI\n",
    "# 1.convert the image to gray scale\n",
    "# 2.use the preprogrammed xml to identify the faces\n",
    "# 3.pass the image to preprogrammed cascade using detectMultiScale\n",
    "# 4.If there are multiple people in the picture we are selecting the first one only\n",
    "# 5.Crop the image top of head left(x) to top of head right(x+w) & bottom of head left(y) to top of head left(y+h)\n",
    "#This makes a square patter w mean width and h means height x and y are the cordinates in the graph\n",
    "def detect_face(img):\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade=cv2.CascadeClassifier(r'C:\\Users\\febia\\OneDrive\\Desktop\\Document\\Others\\Mariam\\Learning\\MachineLearning\\DL Projects\\face recognition\\haarcascade_frontalface_default.xml')\n",
    "    face=face_cascade.detectMultiScale(gray,minNeighbors=4)\n",
    "    #Crop the image with the x,y,w,h values\n",
    "    if len(face)==0:\n",
    "        return None,None#if no value is returned it will create an error so this helps not to return any error\n",
    "    x,y,w,h=face[0]\n",
    "    return gray[x:x+w,y:y+h],face[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "faces=[]\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try yo fetch the training pictures from the sub folders\n",
    "def prepare_training_data(data_folder):\n",
    "    dirs=os.listdir(data_folder)\n",
    "    # print(dirs)\n",
    "\n",
    "    for dir_names in dirs:\n",
    "        sub_dir=data_folder+\"/\"+dir_names\n",
    "        # print(sub_dir)\n",
    "        images=os.listdir(sub_dir)\n",
    "        # print(images)\n",
    "        for img in images:\n",
    "            imgpath=sub_dir+\"/\"+img\n",
    "            # print(imgpath)\n",
    "            image=cv2.imread(imgpath)\n",
    "            cv2.imshow('training images',image)\n",
    "            cv2.waitKey(500)\n",
    "            \n",
    "#For each picture we get ge the cropped image to faces list and append the corresponding labels\n",
    "            #detect face\n",
    "            face,rect=detect_face(image)\n",
    "            if face is not None:\n",
    "                faces.append(face)\n",
    "                labels.append(name_map(dir_names))\n",
    "                # print(labels)\n",
    "    cv2.waitKey(1)            \n",
    "    cv2.destroyAllWindows()\n",
    "    return faces,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "faces,labels=prepare_training_data(r'C:\\Users\\febia\\OneDrive\\Desktop\\Document\\Others\\Mariam\\Learning\\MachineLearning\\DL Projects\\face recognition\\DB')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)#actually 30 \n",
    "#only 29 came as Sachines one face not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pip install --user opencv-contrib-python\n",
    "# https://pyimagesearch.com/2021/05/03/face-recognition-with-local-binary-patterns-lbps-and-opencv/#The%20Local%20Binary%20Patterns%20(LBPS)%20For%20Face%20Recognition%20Algorithm\n",
    "# Once a face has been detected in an image, the first step is to divide the face ROI into 7×7 equally sized cells.\n",
    "# Then, for each of these cells, we compute a Local Binary Pattern histogram.\n",
    "# This spatial encoding also allows us to weigh the resulting histograms from each of the cells differently\n",
    "# , giving more discriminative power to more distinguishing features of the face:\n",
    "# Finally, the weighted 7×7 LBP histograms are concatenated together to form the final feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing face recognition is done using the x^2 distance and a nearest neighbor classifier:\n",
    "\n",
    "# 1.A face is presented to the system\n",
    "# 2.LBPs are extracted, weighted, and concatenated in the same manner as the training data\n",
    "# 3.k-NN (with k=1) is performed with the x^2 distance to find the closest face in the training data.\n",
    "# 4.The name of the person associated with the face with the smallest x^2 distance is chosen as the final classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.face.LBPHFaceRecognizer 000001706AC505B0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer=cv2.face.LBPHFaceRecognizer_create()#Local Binary Pattern Histogram\n",
    "recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are trying to train the faces with LBPHFaceRecognizer\n",
    "import numpy as np\n",
    "recognizer.train(faces,np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rect(img,rect):\n",
    "    (x,y,w,h)=rect\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,0),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(img,text,x,y):\n",
    "    cv2.putText(img,text,(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are trying to predict a face\n",
    "# 1.input the faces\n",
    "# 2.LBPH used to predict\n",
    "# 3.Get the name to write on the predicted Image\n",
    "# 4.Draw a rectangle in the ROI\n",
    "def predict_new(test_img):\n",
    "    face,rect=detect_face(test_img)\n",
    "    label=recognizer.predict(face)\n",
    "    label_text=get_name(label[0])\n",
    "    write_txt(test_img,label_text,rect[0],rect[1])\n",
    "    draw_rect(test_img,rect)\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m predicted \u001b[38;5;241m=\u001b[39m predict_new(test_img)\n\u001b[0;32m      3\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m,predicted)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_img=cv2.imread(r'C:\\Users\\febia\\OneDrive\\Desktop\\Document\\Others\\Mariam\\Learning\\MachineLearning\\DL Projects\\face recognition\\WhatsApp Image 2024-01-02 at 18.54.17_8f7fa609.jpg')\n",
    "predicted = predict_new(test_img)\n",
    "cv2.imshow(\"IMAGE\",predicted)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
